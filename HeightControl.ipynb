{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ruihanz\\Anaconda3\\envs\\PY3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from DDPG.DDPG import DDPG_agent\n",
    "from env.EnvHeightControl import EnvHeightControl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(22)\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for connection: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = EnvHeightControl()\n",
    "state_shape = 1\n",
    "action_bound = 1\n",
    "action_dim = 1\n",
    "agent = DDPG_agent(sess, state_shape, action_bound, action_dim)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.load(saver,\"temp/premodel1/\")\n",
    "#init = tf.variables_initializer(var_list, name='init')  \n",
    "#sess.run(init)\n",
    "#from DDPG.Critic import Critic\n",
    "#agent.critic = Critic(sess, agent.state_shape, agent.action_dim, agent.minibatch_size,lr = 0.001, tau=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nepisode_count, success, episode_reward, step_count = 0, 0, 0, 0\\nstate = env.reset()\\nwhile True:\\n\\n    action = agent.act(state)\\n    next_state, reward, done, info = env.step(action)\\n    reward = (-abs(next_state[1]*50+5) +5) / 50 * 2\\n    reward = reward[0]\\n    episode_reward += reward\\n    agent.observe(state, action, reward, next_state, done)\\n    agent.train(times = 2)\\n    state = next_state\\n    step_count += 1\\n    print (\"aim height: {}\".format(env.aim_height).ljust(20,\" \"),\"abso height: {:.2f}\".format(state[1][0]*50).ljust(20,\" \"), \"reward: {:.5f}.\".format(reward).ljust(20,\" \"),\"steps: {}\".format(step_count).ljust(20,\" \"),end = \"\\r\")\\n\\n    if done:\\n\\n        if info == \"success\":\\n            success += 1\\n        print (\" \"*80,end = \"\\r\")\\n        print(\"episode {} finish, average reward: {:.5f}, total success: {} result: {} step: {}\".format(episode_count, episode_reward/step_count, success, info, step_count).ljust(80,\" \"))\\n        episode_reward = 0\\n        step_count = 0\\n        episode_count += 1\\n        #if episode_count % 100 == 0:\\n        #    agent.save(saver,\"temp/DDPG_4_29_s1/\")\\n        if episode_count >= 100:\\n            break\\n        state = env.reset()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "episode_count, success, episode_reward, step_count = 0, 0, 0, 0\n",
    "state = env.reset()\n",
    "while True:\n",
    "\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    reward = (-abs(next_state[1]*50+5) +5) / 50 * 2\n",
    "    reward = reward[0]\n",
    "    episode_reward += reward\n",
    "    agent.observe(state, action, reward, next_state, done)\n",
    "    agent.train(times = 2)\n",
    "    state = next_state\n",
    "    step_count += 1\n",
    "    print (\"aim height: {}\".format(env.aim_height).ljust(20,\" \"),\"abso height: {:.2f}\".format(state[1][0]*50).ljust(20,\" \"), \"reward: {:.5f}.\".format(reward).ljust(20,\" \"),\"steps: {}\".format(step_count).ljust(20,\" \"),end = \"\\r\")\n",
    "\n",
    "    if done:\n",
    "\n",
    "        if info == \"success\":\n",
    "            success += 1\n",
    "        print (\" \"*80,end = \"\\r\")\n",
    "        print(\"episode {} finish, average reward: {:.5f}, total success: {} result: {} step: {}\".format(episode_count, episode_reward/step_count, success, info, step_count).ljust(80,\" \"))\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        episode_count += 1\n",
    "        #if episode_count % 100 == 0:\n",
    "        #    agent.save(saver,\"temp/DDPG_4_29_s1/\")\n",
    "        if episode_count >= 100:\n",
    "            break\n",
    "        state = env.reset()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom DDPG.OUNoise import OrnsteinUhlenbeckActionNoise\\nfrom DDPG.ReplayMemory import ReplayMemory\\n#agent.replay_memory.load(\"temp/DDPG_4_29_s1/\")\\n#agent.num_action_taken = agent.train_after+1\\n#agent.train(times = 2000)\\nagent.replay_memory.save(\"temp/DDPG_4_29_s1/\")\\nagent.replay_memory = ReplayMemory(agent.memory_size)\\nagent.action_noise = OrnsteinUhlenbeckActionNoise(np.zeros(agent.action_dim))\\nagent.num_action_taken = 0\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from DDPG.OUNoise import OrnsteinUhlenbeckActionNoise\n",
    "from DDPG.ReplayMemory import ReplayMemory\n",
    "#agent.replay_memory.load(\"temp/DDPG_4_29_s1/\")\n",
    "#agent.num_action_taken = agent.train_after+1\n",
    "#agent.train(times = 2000)\n",
    "agent.replay_memory.save(\"temp/DDPG_4_29_s1/\")\n",
    "agent.replay_memory = ReplayMemory(agent.memory_size)\n",
    "agent.action_noise = OrnsteinUhlenbeckActionNoise(np.zeros(agent.action_dim))\n",
    "agent.num_action_taken = 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nepisode_count, success, episode_reward, step_count = 0, 0, 0, 0\\nstate = env.reset()\\nwhile True:\\n\\n    action = agent.act(state)\\n    next_state, reward, done, info = env.step(action)\\n    #print(reward)\\n    episode_reward += reward\\n    agent.observe(state, action, reward, next_state, done)\\n    agent.train(times = 2)\\n    state = next_state\\n    step_count += 1\\n    print (\"aim height: {}\".format(env.aim_height).ljust(20,\" \"),\"abso height: {:.2f}\".format(state[1][0]*50).ljust(20,\" \"), \"reward: {:.5f}.\".format(reward).ljust(20,\" \"),\"steps: {}\".format(step_count).ljust(20,\" \"),end = \"\\r\")\\n\\n    if done:\\n\\n        if info == \"success\":\\n            success += 1\\n        print (\" \"*80,end = \"\\r\")\\n        print(\"episode {} finish, average reward: {:.5f}, total success: {} result: {} step: {}\".format(episode_count, episode_reward/step_count, success, info, step_count).ljust(80,\" \"))\\n        episode_reward = 0\\n        step_count = 0\\n        episode_count += 1\\n        if episode_count % 100 == 0:\\n            agent.save(saver,\"temp/DDPG_4_29_s2/\")\\n        if episode_count >= 400:\\n            break\\n        state = env.reset()\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "episode_count, success, episode_reward, step_count = 0, 0, 0, 0\n",
    "state = env.reset()\n",
    "while True:\n",
    "\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    #print(reward)\n",
    "    episode_reward += reward\n",
    "    agent.observe(state, action, reward, next_state, done)\n",
    "    agent.train(times = 2)\n",
    "    state = next_state\n",
    "    step_count += 1\n",
    "    print (\"aim height: {}\".format(env.aim_height).ljust(20,\" \"),\"abso height: {:.2f}\".format(state[1][0]*50).ljust(20,\" \"), \"reward: {:.5f}.\".format(reward).ljust(20,\" \"),\"steps: {}\".format(step_count).ljust(20,\" \"),end = \"\\r\")\n",
    "\n",
    "    if done:\n",
    "\n",
    "        if info == \"success\":\n",
    "            success += 1\n",
    "        print (\" \"*80,end = \"\\r\")\n",
    "        print(\"episode {} finish, average reward: {:.5f}, total success: {} result: {} step: {}\".format(episode_count, episode_reward/step_count, success, info, step_count).ljust(80,\" \"))\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        episode_count += 1\n",
    "        if episode_count % 100 == 0:\n",
    "            agent.save(saver,\"temp/DDPG_4_29_s2/\")\n",
    "        if episode_count >= 400:\n",
    "            break\n",
    "        state = env.reset()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nepisode_count, success, episode_reward, step_count = 0, 0, 0, 0\\nstate = env.reset()\\nwhile True:\\n\\n    action = agent.act(state)\\n    next_state, reward, done, info = env.step(action)\\n    #print(reward)\\n    episode_reward += reward\\n    agent.observe(state, action, reward, next_state, done)\\n    agent.train(times = 2)\\n    state = next_state\\n    step_count += 1\\n    print (\"aim height: {}\".format(env.aim_height).ljust(20,\" \"),\"abso height: {:.2f}\".format(state[1][0]*50).ljust(20,\" \"), \"reward: {:.5f}.\".format(reward).ljust(20,\" \"),\"steps: {}\".format(step_count).ljust(20,\" \"),end = \"\\r\")\\n\\n    if done:\\n\\n        if info == \"success\":\\n            success += 1\\n        print (\" \"*80,end = \"\\r\")\\n        print(\"episode {} finish, average reward: {:.5f}, total success: {} result: {} step: {}\".format(episode_count, episode_reward/step_count, success, info, step_count).ljust(80,\" \"))\\n        episode_reward = 0\\n        step_count = 0\\n        episode_count += 1\\n        if episode_count % 100 == 0:\\n            agent.save(saver,\"temp/DDPG_4_29_s3/\")\\n        if episode_count >= 40000:\\n            break\\n        state = env.reset()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "episode_count, success, episode_reward, step_count = 0, 0, 0, 0\n",
    "state = env.reset()\n",
    "while True:\n",
    "\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    #print(reward)\n",
    "    episode_reward += reward\n",
    "    agent.observe(state, action, reward, next_state, done)\n",
    "    agent.train(times = 2)\n",
    "    state = next_state\n",
    "    step_count += 1\n",
    "    print (\"aim height: {}\".format(env.aim_height).ljust(20,\" \"),\"abso height: {:.2f}\".format(state[1][0]*50).ljust(20,\" \"), \"reward: {:.5f}.\".format(reward).ljust(20,\" \"),\"steps: {}\".format(step_count).ljust(20,\" \"),end = \"\\r\")\n",
    "\n",
    "    if done:\n",
    "\n",
    "        if info == \"success\":\n",
    "            success += 1\n",
    "        print (\" \"*80,end = \"\\r\")\n",
    "        print(\"episode {} finish, average reward: {:.5f}, total success: {} result: {} step: {}\".format(episode_count, episode_reward/step_count, success, info, step_count).ljust(80,\" \"))\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        episode_count += 1\n",
    "        if episode_count % 100 == 0:\n",
    "            agent.save(saver,\"temp/DDPG_4_29_s3/\")\n",
    "        if episode_count >= 40000:\n",
    "            break\n",
    "        state = env.reset()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp/premodel2\\model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.load(saver,\"temp/premodel2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 finish, average reward: 0.03095, total success: 1 result: success step: 67\n",
      "episode 1 finish, average reward: 0.03049, total success: 2 result: success step: 67\n",
      "episode 2 finish, average reward: 0.03075, total success: 3 result: success step: 66\n",
      "episode 3 finish, average reward: 0.02996, total success: 4 result: success step: 69\n",
      "episode 4 finish, average reward: 0.03013, total success: 5 result: success step: 68\n",
      "episode 5 finish, average reward: 0.03013, total success: 6 result: success step: 68\n",
      "episode 6 finish, average reward: 0.03051, total success: 7 result: success step: 67\n",
      "episode 7 finish, average reward: 0.03036, total success: 8 result: success step: 67\n",
      "episode 8 finish, average reward: 0.03040, total success: 9 result: success step: 67\n",
      "episode 9 finish, average reward: 0.03042, total success: 10 result: success step: 67\n",
      "episode 10 finish, average reward: 0.03037, total success: 11 result: success step: 67\n",
      "episode 11 finish, average reward: 0.03055, total success: 12 result: success step: 67\n",
      "episode 12 finish, average reward: 0.03034, total success: 13 result: success step: 67\n",
      "episode 13 finish, average reward: 0.03043, total success: 14 result: success step: 67\n",
      "episode 14 finish, average reward: 0.03035, total success: 15 result: success step: 67\n",
      "episode 15 finish, average reward: 0.03043, total success: 16 result: success step: 67\n",
      "episode 16 finish, average reward: 0.03047, total success: 17 result: success step: 67\n",
      "episode 17 finish, average reward: 0.03046, total success: 18 result: success step: 67\n",
      "episode 18 finish, average reward: 0.03036, total success: 19 result: success step: 67\n",
      "episode 19 finish, average reward: 0.03040, total success: 20 result: success step: 67\n",
      "episode 20 finish, average reward: 0.03047, total success: 21 result: success step: 67\n",
      "episode 21 finish, average reward: 0.03062, total success: 22 result: success step: 67\n",
      "episode 22 finish, average reward: 0.03038, total success: 23 result: success step: 67\n",
      "episode 23 finish, average reward: 0.03025, total success: 24 result: success step: 68\n",
      "episode 24 finish, average reward: 0.03041, total success: 25 result: success step: 67\n",
      "episode 25 finish, average reward: 0.03052, total success: 26 result: success step: 67\n",
      "episode 26 finish, average reward: 0.03053, total success: 27 result: success step: 67\n",
      "episode 27 finish, average reward: 0.03042, total success: 28 result: success step: 67\n",
      "episode 28 finish, average reward: 0.03054, total success: 29 result: success step: 67\n",
      "episode 29 finish, average reward: 0.03062, total success: 30 result: success step: 67\n",
      "episode 30 finish, average reward: 0.03049, total success: 31 result: success step: 67\n",
      "episode 31 finish, average reward: 0.03048, total success: 32 result: success step: 67\n",
      "aim height: -10      abso height: -4.71   reward: 0.01174.     steps: 55           \r"
     ]
    }
   ],
   "source": [
    "episode_count, success, episode_reward, step_count = 0, 0, 0, 0\n",
    "state = env.reset()\n",
    "while True:\n",
    "\n",
    "    action = agent.act(state,noise = False)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    #print(reward)\n",
    "    episode_reward += reward\n",
    "    state = next_state\n",
    "    step_count += 1\n",
    "    print (\"aim height: {}\".format(env.aim_height).ljust(20,\" \"),\"abso height: {:.2f}\".format(state[1][0]*50).ljust(20,\" \"), \"reward: {:.5f}.\".format(reward).ljust(20,\" \"),\"steps: {}\".format(step_count).ljust(20,\" \"),end = \"\\r\")\n",
    "\n",
    "    if done:\n",
    "\n",
    "        if info == \"success\":\n",
    "            success += 1\n",
    "        print (\" \"*80,end = \"\\r\")\n",
    "        print(\"episode {} finish, average reward: {:.5f}, total success: {} result: {} step: {}\".format(episode_count, episode_reward/step_count, success, info, step_count).ljust(80,\" \"))\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        episode_count += 1\n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
