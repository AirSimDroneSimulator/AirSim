{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L:\\Anaconda3\\envs\\PT\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import env.AirSimClient as AirSimClient\n",
    "import time\n",
    "from env.EnvImgConrtol import EnvImgControl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IMGDQN.RL_net import DQNClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IMGDQN.model import get_model\n",
    "test_model = get_model(2,0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {}\n",
    "\n",
    "settings[\"learning_rate\"] = 0.003\n",
    "settings[\"reward_decay\"] = 0.9\n",
    "settings[\"e_greedy\"] = 0.99\n",
    "settings[\"e_greedy_increasement\"] = 0.0003\n",
    "settings[\"memory_size\"] = 81920\n",
    "settings[\"batch_size\"] = 32\n",
    "settings[\"epochs\"] = 1\n",
    "settings[\"replace_target_iter\"] = 50\n",
    "settings[\"model\"] = test_model\n",
    "settings[\"n_actions\"] = 2\n",
    "\n",
    "#settings[\"model\"] = keras.models.load_model(\"temp/IMG.h5\")\n",
    "RL = DQNClass(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for connection: \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 10)        122890    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 40960)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 81922     \n",
      "=================================================================\n",
      "Total params: 204,812\n",
      "Trainable params: 204,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = EnvImgControl()\n",
    "RL.model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset due to  collision  Episode reward:  -72.2205810546875 total success: 0 of 1  \n",
      "reset due to  collision  Episode reward:  -74.8446044921875 total success: 0 of 2  \n",
      "action: 0            height: -9.96        reward: 1.11670.     steps: 3            \r"
     ]
    }
   ],
   "source": [
    "episode_step = 0\n",
    "total_step = 0\n",
    "episode = 0\n",
    "episode_reward = 0\n",
    "total_success = 0\n",
    "reward_list = []\n",
    "\n",
    "s = env.reset()\n",
    "while True:\n",
    "    \n",
    "    action = RL.choose_action(s)\n",
    "    \n",
    "    s_,r,done,info = env.step(action)\n",
    "    \n",
    "    #env.render(extra1 = str(count),extra2 = \"reward: \"+str(int(r))+\" Done: \"+str(succ)+\" Episode: \"+str(episode))\n",
    "    print (\"action: {}\".format(action).ljust(20,\" \"),\"height: {:.2f}\".format(env.getPos()[2]).ljust(20,\" \"), \"reward: {:.5f}.\".format(r).ljust(20,\" \"),\"steps: {}\".format(episode_step).ljust(20,\" \"),end = \"\\r\")\n",
    "    \n",
    "    episode_step += 1\n",
    "    total_step += 1\n",
    "    episode_reward += r\n",
    "    RL.add_data(s, action, r, s_,done)\n",
    "    \n",
    "    if done:\n",
    "        if info == \"success\":\n",
    "            total_success += 1\n",
    "        episode += 1\n",
    "        RL.model.save(\"temp/temp.h5\")\n",
    "        reward_list.append(episode_reward)\n",
    "        print (\" \"*80,end = '\\r')\n",
    "        print (\"reset due to \",info,\" Episode reward: \",episode_reward,\"total success: {} of {}\".format(total_success,episode))\n",
    "        episode_step = 0\n",
    "        episode_reward = 0\n",
    "        if episode >= 5000:\n",
    "            break\n",
    "        env.reset()\n",
    "        \n",
    "    if total_step > 1:\n",
    "        RL.learn(times = 2)\n",
    "    \n",
    "\n",
    "        \n",
    "    s = s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL.model.predict(np.array([s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RL.model.save(\"temp/IMG_r+-1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EnvImgControl(aim = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_step = 0\n",
    "total_step = 0\n",
    "episode = 0\n",
    "episode_reward = 0\n",
    "total_success = 0\n",
    "reward_list = []\n",
    "\n",
    "s = env.reset()\n",
    "while True:\n",
    "    \n",
    "    action = RL.choose_action(s)\n",
    "    \n",
    "    s_,r,done,info = env.step(action)\n",
    "    \n",
    "    #env.render(extra1 = str(count),extra2 = \"reward: \"+str(int(r))+\" Done: \"+str(succ)+\" Episode: \"+str(episode))\n",
    "    print (\"action: {}\".format(action).ljust(20,\" \"),\"height: {:.2f}\".format(env.getPos()[2]).ljust(20,\" \"), \"reward: {:.5f}.\".format(r).ljust(20,\" \"),\"steps: {}\".format(episode_step).ljust(20,\" \"),end = \"\\r\")\n",
    "    \n",
    "    episode_step += 1\n",
    "    total_step += 1\n",
    "    episode_reward += r\n",
    "    RL.add_data(s, action, r, s_,done)\n",
    "    \n",
    "    if done:\n",
    "        if info == \"success\":\n",
    "            total_success += 1\n",
    "        episode += 1\n",
    "        RL.model.save(\"temp/temp1.h5\")\n",
    "        reward_list.append(episode_reward)\n",
    "        print (\" \"*80,end = '\\r')\n",
    "        print (\"reset due to \",info,\" Episode reward: \",episode_reward,\"total success: {} of {}\".format(total_success,episode))\n",
    "        episode_step = 0\n",
    "        episode_reward = 0\n",
    "        if episode >= 500:\n",
    "            break\n",
    "        env.reset()\n",
    "        \n",
    "    if total_step > 1:\n",
    "        RL.learn(times = 2)\n",
    "    \n",
    "\n",
    "        \n",
    "    s = s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
