{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ruihanz\\Anaconda3\\envs\\PY3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import AirSimClient\n",
    "import time\n",
    "import drone_env\n",
    "\n",
    "from RL_net import DQNClass\n",
    "from RL_dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "test_model = None\n",
    "#test_model = keras.models.load_model(\"finala.h5\")\n",
    "test_model = keras.models.load_model(\"finalb.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {}\n",
    "\n",
    "settings[\"learning_rate\"] = 0.003\n",
    "settings[\"reward_decay\"] = 0.9\n",
    "settings[\"e_greedy\"] = 0.99\n",
    "settings[\"e_greedy_increasement\"] = 0\n",
    "settings[\"memory_length\"] = 81920\n",
    "settings[\"batch_size\"] = 32\n",
    "settings[\"epochs\"] = 1\n",
    "settings[\"replace_target_iter\"] = 50\n",
    "settings[\"model\"] = test_model\n",
    "settings[\"n_actions\"] = 7\n",
    "settings[\"n_features\"] = 6\n",
    "\n",
    "aim = AirSimClient.Vector3r(32,38,-4)\n",
    "start = AirSimClient.Vector3r(0,0,-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for connection: \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               700       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 31,707\n",
      "Trainable params: 31,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "RL = DQNClass(settings)\n",
    "env = drone_env.drone_env_gridworld()\n",
    "RL.model.summary()\n",
    "\n",
    "\n",
    "\n",
    "current = 0\n",
    "count = 0\n",
    "learns = 0\n",
    "epochs = 0\n",
    "succ = 0\n",
    "learn = 0\n",
    "treward = []\n",
    "rc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 distance: 69 position: [ 2  0 -6] reward: 0 Done: 6 Epochs: 6\n",
      "target net replaced\n",
      "102 distance: 64 position: [ 7  0 -6] reward: 4 Done: 6 Epochs: 6\n",
      "103 distance: 59 position: [12  0 -6] reward: 4 Done: 6 Epochs: 6\n",
      "104 distance: 55 position: [15  2 -7] reward: 3 Done: 6 Epochs: 6\n",
      "105 distance: 50 position: [15  7 -7] reward: 3 Done: 6 Epochs: 6\n",
      "106 distance: 45 position: [15 12 -7] reward: 3 Done: 6 Epochs: 6\n",
      "107 distance: 40 position: [15 17 -7] reward: 4 Done: 6 Epochs: 6\n",
      "108 distance: 35 position: [15 22 -7] reward: 4 Done: 6 Epochs: 6\n",
      "109 distance: 30 position: [15 27 -7] reward: 4 Done: 6 Epochs: 6\n",
      "110 distance: 26 position: [17 30 -8] reward: 2 Done: 6 Epochs: 6\n",
      "111 distance: 21 position: [22 30 -8] reward: 3 Done: 6 Epochs: 6\n",
      "112 distance: 17 position: [25 32 -9] reward: 3 Done: 6 Epochs: 6\n",
      "113 distance: 12 position: [25 36 -8] reward: 4 Done: 6 Epochs: 6\n",
      "114 distance: 7 position: [27 37 -6] reward: 100 Done: 6 Epochs: 6\n",
      "reset due to success epoch reward: 142.0282461521881\n",
      "--------------------------------reset---------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-13ccff30df65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0ms_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextra1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mextra2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"reward: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" Done: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msucc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" Epochs: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\googledrive\\2018-Semester1\\COMP90055 Computing Project\\DQN\\drone_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpret_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mdrone_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveByDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mpos_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv2t\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPosition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\googledrive\\2018-Semester1\\COMP90055 Computing Project\\DQN\\drone_env.py\u001b[0m in \u001b[0;36mmoveByDist\u001b[1;34m(self, diff, forward)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveByVelocity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mdrivetrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAirSimClient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDrivetrainType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mForwardOnly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myaw_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "s = env.reset()\n",
    "while True:\n",
    "    \n",
    "    action = RL.choose_action(s)\n",
    "    \n",
    "    s_,r,done,info = env.step(action)\n",
    "    \n",
    "    env.render(extra1 = str(count),extra2 = \"reward: \"+str(int(r))+\" Done: \"+str(succ)+\" Epochs: \"+str(epochs))\n",
    "      \n",
    "    count += 1\n",
    "    current += 1\n",
    "    if info == \"success\":\n",
    "        succ += 1\n",
    "    if current>100:\n",
    "        done = True\n",
    "        info = \"out of steps\"\n",
    "    \n",
    "    rc += r\n",
    "    counter = RL.add_data(s, action, r, s_)\n",
    "    \n",
    "    if done:\n",
    "        current = 0\n",
    "        epochs += 1\n",
    "        RL.model.save(\"test.h5\")\n",
    "        treward.append(rc)\n",
    "        print (\"reset due to\",info,\"epoch reward:\",rc)\n",
    "        print (\"--------------------------------reset---------------------------\")\n",
    "        rc = 0\n",
    "        env.reset()\n",
    "        \n",
    "    if count > 1:\n",
    "        RL.learn(times = 2)\n",
    "        learn += 1\n",
    "        \n",
    "    s = s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL.model.predict(np.array([[0,-2, -11, 0,-4, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL.model.predict(np.array([[0,30, -5, 0,0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (treward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del env\n",
    "env = drone_env.drone_env_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
